{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8746d3",
   "metadata": {},
   "source": [
    "## AIDI 1002 Final Project: An Empirical Analysis of DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b931fa",
   "metadata": {},
   "source": [
    "\n",
    "### Reproduction, Generalization, and Baseline Comparison\n",
    "\n",
    "**Group Members:** Srikrishna Thapa, Prem Prasad Bhatta\n",
    "**Original Paper:** *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter* (Sanh et al., 2019)\n",
    "\n",
    "### 1. Project Goal\n",
    "\n",
    "The goal of this project is to first reproduce the text classification performance of the DistilBERT model on the IMDb sentiment analysis dataset, as reported in the original paper.\n",
    "\n",
    "Following the successful reproduction, we make two significant contributions to extend the analysis:\n",
    "\n",
    "1.  **To Test Generalization:** We evaluate the same DistilBERT methodology on a new dataset, **SST-2 (Stanford Sentiment Treebank)**, to assess its effectiveness in a different context with shorter text inputs.\n",
    "\n",
    "2.  **To Analyze Efficiency:** We compare DistilBERT's performance against a fast, classical machine learning baseline (**Logistic Regression with TF-IDF features**) on the original IMDb dataset.\n",
    "\n",
    "This three-part experiment allows us to not only validate the paper's findings but also to critically analyze the model's generalization capabilities and its performance trade-offs against simpler, more efficient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## %pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\srikr\\onedrive\\desktop\\data_projects\\bert-vs-distilbert\\gpu_env_311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    " ## %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5956ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srikr\\OneDrive\\Desktop\\Data_Projects\\bert-vs-distilbert\\gpu_env_311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed01c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Testing model: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Set the model we want to test. Change this to \"bert-base-uncased\" for the comparison run.\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\" \n",
    "# MODEL_CHECKPOINT = \"bert-base-uncased\" \n",
    "\n",
    "# Other settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # As recommended for fine-tuning\n",
    "EPOCHS = 2      # Use 2-3 epochs for fine-tuning\n",
    "MAX_LENGTH = 512 # Max length for tokenization\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Testing model: {MODEL_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdf92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDb dataset...\n",
      "Dataset loaded.\n",
      "Training samples: 25000, Test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset\n",
    "print(\"Loading IMDb dataset...\")\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# A smaller subset for faster testing/debugging if needed.\n",
    "# For the final run, use the full dataset.\n",
    "# train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Use the full dataset for the final experiment\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e36daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the dataset... This may take a few minutes.\n",
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for the chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"Tokenizing the dataset... This may take a few minutes.\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased\n",
      "Number of trainable parameters: 66.96M\n"
     ]
    }
   ],
   "source": [
    "# Load the model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
    "model.to(DEVICE) # Move model to GPU if available\n",
    "\n",
    "# Function to count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Model loaded: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Number of trainable parameters: {num_params / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2728cdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cef9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ca8594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Define training arguments for transformers v4.35.2\n",
    "# This is the correct, modern syntax.\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{MODEL_CHECKPOINT}-imdb\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",  # Use eval_strategy instead\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "print(\"TrainingArguments configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd71578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 56:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.243904</td>\n",
       "      <td>0.903760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.326173</td>\n",
       "      <td>0.887560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.225185</td>\n",
       "      <td>0.918360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.287646</td>\n",
       "      <td>0.923760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.139900</td>\n",
       "      <td>0.234382</td>\n",
       "      <td>0.931720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.242869</td>\n",
       "      <td>0.932240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Total training time: 56.80 minutes\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training and time it\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training complete.\")\n",
    "print(f\"Total training time: {training_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca726b",
   "metadata": {},
   "source": [
    "### 2. Experimental Results\n",
    "After training, we evaluate the model on the test set to get our final accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d458c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the final model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 04:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results ---\n",
      "Model: distilbert-base-uncased\n",
      "Accuracy: 0.9184\n",
      "Training Time: 56.80 minutes\n",
      "Number of Parameters: 66.96M\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating the final model on the test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the results in a clean format\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(f\"Model: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Training Time: {training_time / 60:.2f} minutes\")\n",
    "print(f\"Number of Parameters: {num_params / 1_000_000:.2f}M\")\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfaebae",
   "metadata": {},
   "source": [
    "### Reproduction Summary\n",
    "\n",
    "We successfully reproduced the paper's findings by fine-tuning DistilBERT on the IMDb dataset. Our experiment yielded the following results:\n",
    "\n",
    "- **Final Accuracy:** 91.84%\n",
    "- **Training Time:** 56.80 minutes\n",
    "- **Model Size:** 66.96M Parameters\n",
    "\n",
    "This result is strong and consistent with the performance reported in the original paper. We will now proceed with our two significant contributions to extend this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c094fcc",
   "metadata": {},
   "source": [
    "## Contribution 1: Evaluating Model Generalization on the SST-2 Dataset\n",
    "\n",
    "Our first contribution tests the DistilBERT methodology on a new dataset to evaluate its effectiveness in a different context. We will fine-tune the same model on the **SST-2 dataset**, which consists of shorter, single-sentence movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ed7ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset from GLUE benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on SST-2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12630/12630 38:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SST-2 Experiment Results ---\n",
      "Model: distilbert-base-uncased\n",
      "Accuracy on SST-2: 0.9106\n",
      "Training Time: 38.38 minutes\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CONTRIBUTION 1: SST-2 EXPERIMENT ---\n",
    "\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np # Make sure numpy is imported\n",
    "\n",
    "# --- Configuration for this specific experiment ---\n",
    "MODEL_CHECKPOINT_SST2 = \"distilbert-base-uncased\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 \n",
    "EPOCHS = 3 \n",
    "MAX_LENGTH = 128\n",
    "\n",
    "#  Load the SST-2 dataset\n",
    "print(\"Loading SST-2 dataset from GLUE benchmark...\")\n",
    "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_dataset_sst2 = sst2_dataset[\"train\"]\n",
    "eval_dataset_sst2 = sst2_dataset[\"validation\"]\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer_sst2 = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT_SST2)\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer_sst2(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "tokenized_train_sst2 = train_dataset_sst2.map(tokenize_sst2, batched=True)\n",
    "tokenized_eval_sst2 = eval_dataset_sst2.map(tokenize_sst2, batched=True)\n",
    "\n",
    "# Load the model\n",
    "model_sst2 = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT_SST2, num_labels=2)\n",
    "model_sst2.to(DEVICE)\n",
    "\n",
    "# Define a metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "#  Set up Trainer with the SIMPLIFIED, COMPATIBLE arguments\n",
    "training_args_sst2 = TrainingArguments(\n",
    "    output_dir=\"./results/distilbert-sst2\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps=100,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer_sst2 = Trainer(\n",
    "    model=model_sst2,\n",
    "    args=training_args_sst2,\n",
    "    train_dataset=tokenized_train_sst2,\n",
    "    eval_dataset=tokenized_eval_sst2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. Train and evaluate\n",
    "print(\"\\nStarting training on SST-2...\")\n",
    "start_time_sst2 = time.time()\n",
    "trainer_sst2.train()\n",
    "end_time_sst2 = time.time()\n",
    "training_time_sst2 = end_time_sst2 - start_time_sst2\n",
    "\n",
    "# We manually evaluate since we removed the evaluation strategy during training\n",
    "eval_results_sst2 = trainer_sst2.evaluate()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- SST-2 Experiment Results ---\")\n",
    "print(f\"Model: {MODEL_CHECKPOINT_SST2}\")\n",
    "print(f\"Accuracy on SST-2: {eval_results_sst2['eval_accuracy']:.4f}\")\n",
    "print(f\"Training Time: {training_time_sst2 / 60:.2f} minutes\")\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335e135",
   "metadata": {},
   "source": [
    "---\n",
    "## Contribution 2: Performance vs. a Classical Baseline on IMDb\n",
    "\n",
    "Our second contribution analyzes if the complexity of a Transformer is necessary for the original IMDb task. We compare DistilBERT's result against a simple and fast **Logistic Regression** model using TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acddac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw IMDb text data...\n",
      "Creating TF-IDF features... (This may take a minute)\n",
      "TF-IDF features created.\n",
      "Training Logistic Regression model...\n",
      "Training complete.\n",
      "\n",
      "--- Classical Model Results ---\n",
      "Model: Logistic Regression with TF-IDF\n",
      "Accuracy on IMDb: 0.8952\n",
      "Training Time: 0.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- CONTRIBUTION 2: CLASSICAL BASELINE EXPERIMENT ---\n",
    "# (This entire experiment runs in one cell and will be very fast)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np # Make sure numpy is imported\n",
    "\n",
    "# 1. Load the raw IMDb text data\n",
    "print(\"Loading raw IMDb text data...\")\n",
    "imdb_raw = load_dataset(\"imdb\")\n",
    "train_texts = [example['text'] for example in imdb_raw['train']]\n",
    "train_labels = [example['label'] for example in imdb_raw['train']]\n",
    "test_texts = [example['text'] for example in imdb_raw['test']]\n",
    "test_labels = [example['label'] for example in imdb_raw['test']]\n",
    "\n",
    "# 2. Create TF-IDF features from the text\n",
    "print(\"Creating TF-IDF features... (This may take a minute)\")\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "print(\"TF-IDF features created.\")\n",
    "\n",
    "# 3. Train the Logistic Regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "start_time_lr = time.time()\n",
    "lr_model = LogisticRegression(max_iter=1000, C=1.0, solver='liblinear')\n",
    "lr_model.fit(X_train, train_labels)\n",
    "end_time_lr = time.time()\n",
    "training_time_lr = end_time_lr - start_time_lr\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 4. Evaluate the model's performance\n",
    "predictions = lr_model.predict(X_test)\n",
    "accuracy_lr = accuracy_score(test_labels, predictions)\n",
    "\n",
    "# 5. Print the final results\n",
    "print(\"\\n--- Classical Model Results ---\")\n",
    "print(f\"Model: Logistic Regression with TF-IDF\")\n",
    "print(f\"Accuracy on IMDb: {accuracy_lr:.4f}\")\n",
    "print(f\"Training Time: {training_time_lr:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01afd1",
   "metadata": {},
   "source": [
    "## Overall Project Conclusion & Summary Table\n",
    "\n",
    "This project successfully reproduced the high performance of DistilBERT and extended the original paper's findings through two distinct contributions, yielding a comprehensive analysis of the model's performance, generalization, and efficiency.\n",
    "\n",
    "The key results from our three experiments are summarized in the table below.\n",
    "\n",
    "| Experiment | Model | Dataset | Accuracy | Training Time |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Reproduction** | DistilBERT | IMDb | **91.84%** | **56.80 mins** |\n",
    "| **Contribution 1** | DistilBERT | SST-2 | **91.06%** | **38.38 mins** |\n",
    "| **Contribution 2** | Logistic Regression | IMDb | **89.52%** | **0.56 seconds** |\n",
    "\n",
    "### Key Findings and Analysis\n",
    "\n",
    "1.  **Successful Reproduction:** Our baseline experiment confirmed that DistilBERT is a powerful model for sentiment analysis, achieving **91.84%** accuracy on the IMDb dataset. This result is consistent with the high performance reported in the original paper.\n",
    "\n",
    "2.  **Excellent Generalization:** Our first contribution tested the model on the SST-2 dataset. It achieved an impressive accuracy of **91.06%**, proving that the DistilBERT methodology is robust and generalizes well to different text formats (long-form vs. short-form sentences).\n",
    "\n",
    "3.  **The Efficiency Trade-Off:** Our second contribution provided a critical perspective. The classical Logistic Regression model, while less accurate at **89.52%**, achieved a very strong result in **under one second**. This highlights the significant trade-off between achieving state-of-the-art performance with a deep learning model versus the immense speed and resource savings of a simpler, classical approach.\n",
    "\n",
    "**Final Conclusion:** Our work validates that DistilBERT is a highly effective model for text classification. However, our comparative analysis demonstrates that for many real-world projects, the choice of model is a crucial business decision. While Transformers offer peak performance, classical baselines remain a powerful and incredibly efficient alternative when development speed and computational cost are the primary concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
